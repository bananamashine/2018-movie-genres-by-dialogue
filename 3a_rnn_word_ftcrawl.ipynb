{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autotime\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import pandas as pd\n",
    "import re, string, multiprocess, pickle, tqdm, gc, warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate\n",
    "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D, CuDNNGRU, Dropout\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
    "from keras.initializers import he_uniform\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = text.strip(' ')\n",
    "    return text \n",
    "\n",
    "def lemmatize_all(sentence):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    for word, tag in pos_tag(word_tokenize(sentence)):\n",
    "        if tag.startswith(\"NN\"):\n",
    "            yield wnl.lemmatize(word, pos='n')\n",
    "        elif tag.startswith('VB'):\n",
    "            yield wnl.lemmatize(word, pos='v')\n",
    "        elif tag.startswith('JJ'):\n",
    "            yield wnl.lemmatize(word, pos='a')\n",
    "        elif tag.startswith('R'):\n",
    "            yield wnl.lemmatize(word, pos='r')\n",
    "        else:\n",
    "            yield word\n",
    "\n",
    "def lemmatize(data):\n",
    "    if __name__ == \"__main__\": \n",
    "        with multiprocess.Pool() as p:\n",
    "            result = list(tqdm.tqdm_notebook(p.imap(lambda x: \" \".join(lemmatize_all(clean_text(str(x)))), \n",
    "                                                    iter(data), chunksize=100), total=len(data)))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMBEDDING_FILE = '../../../Embeddings/EN/BPE/embeddings/en.wiki.bpe.op3000.d300.w2v.txt'\n",
    "EMBEDDING_FILE = '../../../Embeddings/crawl-300d-2M.vec'\n",
    "# EMBEDDING_FILE = '../../../Embeddings/glove.840B.300d.txt'\n",
    "\n",
    "train = pd.read_csv('/home/bananamachine/.kaggle/competitions/movie-genres-by-dialogue/train.csv.zip')\n",
    "test = pd.read_csv('/home/bananamachine/.kaggle/competitions/movie-genres-by-dialogue/X_test.csv.zip')\n",
    "\n",
    "def clean_labels(text):\n",
    "    text = re.sub('-', '', text)\n",
    "    return text \n",
    "\n",
    "train.genres = train.genres.map(lambda x: clean_labels(x))\n",
    "label_vectorizer = CountVectorizer()\n",
    "train_y = label_vectorizer.fit_transform(train.genres).todense()\n",
    "label_cols = label_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dialogue = lemmatize(train.dialogue)\n",
    "test.dialogue = lemmatize(test.dialogue)\n",
    "\n",
    "X_train = train['dialogue'].fillna('fillna').values\n",
    "X_test = test['dialogue'].fillna('fillna').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 100000\n",
    "maxlen = 200\n",
    "embed_size = 300\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(X_train) + list(X_test))\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embeddings_index(embeddings_path, embeddings_size):\n",
    "    index = {}\n",
    "    with open(embeddings_path, 'rb') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            if vector.shape[0] != embeddings_size:\n",
    "                pass\n",
    "            else:\n",
    "                index[word] = vector\n",
    "    return index\n",
    "\n",
    "embeddings_index = build_embeddings_index(EMBEDDING_FILE, embed_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k, x in zip(embeddings_index.keys(), embeddings_index.values()):\n",
    "#     if x.shape[0] != embed_size:\n",
    "#         print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "print(emb_mean, emb_std)\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "print(embedding_matrix.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RocAucEvaluation(Callback):\n",
    "    \n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        if not ('val_auc' in self.params['metrics']):\n",
    "            self.params['metrics'].append('roc_auc_val')        \n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        logs['val_auc'] = float('-inf')\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            logs['val_auc'] = score\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(params):\n",
    "    initializer = he_uniform(seed=0)\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(embedding_matrix.shape[0], embed_size, weights=[embedding_matrix])(inp)\n",
    "    x = SpatialDropout1D(params['sd0'])(x)\n",
    "    x = Bidirectional(CuDNNGRU(params['runits'], recurrent_initializer = initializer, return_sequences=True))(x)\n",
    "    x = Dropout(params['d0'])(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "    outp = Dense(1, activation='sigmoid')(conc)\n",
    "    with tf.device('/cpu:1'):\n",
    "        model = Model(inputs=inp, outputs=outp)    \n",
    "    return model\n",
    "\n",
    "def val_run_model(train_X, train_y, val_X, val_y, test_X, params): \n",
    "\n",
    "    try: pp = multiprocess.current_process()._identity[0]\n",
    "    except IndexError: pp = 0\n",
    "    \n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.Session(config=config) as sess:\n",
    "            K.set_session(sess)\n",
    "            \n",
    "            model = get_model(params)\n",
    "            model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "            \n",
    "            file_path, log_file_path = 'rnna_auc_checkpoint_{}.hdf5'.format(str(pp)), 'rnna_csv_log_{}.csv'.format(str(pp))\n",
    "            rocauc = RocAucEvaluation(validation_data=(val_X, val_y), interval=1)\n",
    "            lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=3, min_lr=0.5e-6)\n",
    "            early_stopper = EarlyStopping(monitor='val_auc', mode='max', patience=6)\n",
    "            csv_logger = CSVLogger(log_file_path)\n",
    "            checkpoint = ModelCheckpoint(file_path, monitor='val_auc', verbose=1, save_best_only=True, mode='max')           \n",
    "            callbacks_list = [rocauc, lr_reducer, early_stopper, csv_logger, checkpoint]\n",
    "            \n",
    "            model.fit(train_X, train_y, batch_size=params['bs'], epochs=40, validation_data=(val_X, val_y), verbose=2, callbacks=callbacks_list)\n",
    "            model.load_weights(file_path)\n",
    "            pred_val_y = model.predict(val_X, batch_size=512)\n",
    "            pred_test_y = model.predict(test_X, batch_size=512)        \n",
    "\n",
    "            sess.close()\n",
    "            del sess, model, lr_reducer, early_stopper, checkpoint, csv_logger; gc.collect();\n",
    "    return pred_val_y, pred_test_y                  \n",
    "  \n",
    "def val_predict_fold(i, dev_index, val_index, params): \n",
    "    pred_train = np.zeros(len(X_train))\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = np.squeeze(np.array(train_y[:, i][dev_index])), np.squeeze(np.array(train_y[:, i][val_index]))\n",
    "    pred_val_y, pred_test_y = val_run_model(dev_X, dev_y, val_X, val_y, x_test, params)\n",
    "    pred_train[val_index] = np.squeeze(np.array(pred_val_y))\n",
    "    cv_scores = roc_auc_score(val_y, pred_val_y)\n",
    "    return cv_scores, pred_train, pred_test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_predict_oof(i, n, params):  \n",
    "    if __name__ == '__main__':\n",
    "        kf = StratifiedKFold(n_splits=n, shuffle=True, random_state=0)\n",
    "        with multiprocess.Pool(n) as p:\n",
    "            r = list(p.imap(lambda x: val_predict_fold(i, x[0], x[1], params), kf.split(train_y[:, i], train_y[:, i]), chunksize=1))\n",
    "    return r, np.mean([x[0] for x in r]), np.sum([x[1] for x in r], axis=0), np.sum([x[2] for x in r], axis=0) / float(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'sd0': 0.2,\n",
    "    'runits': 60,\n",
    "    'd0': 0.2,\n",
    "    'bs': 256,\n",
    "}\n",
    "\n",
    "preds_test = np.zeros((X_test.shape[0], len(label_cols)))\n",
    "preds_train = np.zeros((X_train.shape[0], len(label_cols)))\n",
    "cv_scores = list()\n",
    "\n",
    "for i, j in enumerate(label_cols):\n",
    "    print('cv', j)\n",
    "    r, scores, pred_train, pred_test = val_predict_oof(i, 5, params)\n",
    "    preds_train[:, i] = pred_train\n",
    "    preds_test[:, i] = pred_test.flatten()\n",
    "    cv_scores.append(np.mean(scores))\n",
    "    print(np.mean(scores))\n",
    "    \n",
    "print(np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    train['pred_rnnft0_{}'.format(str(i))] = preds_train[:, i]\n",
    "    test['pred_rnnft0_{}'.format(str(i))] = preds_test[:, i]\n",
    "    \n",
    "train.iloc[:, -20:].to_csv('predictions/pred_train_rnnft0.csv', index=False)\n",
    "test.iloc[:, -20:].to_csv('predictions/pred_test_rnnft0.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
