{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autotime\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import re, multiprocess, gc\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.layers import Dense, GlobalAveragePooling1D, Embedding\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate\n",
    "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D, CuDNNGRU, Dropout\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
    "from keras.initializers import he_uniform\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/home/bananamachine/.kaggle/competitions/movie-genres-by-dialogue/train.csv.zip')\n",
    "test = pd.read_csv('/home/bananamachine/.kaggle/competitions/movie-genres-by-dialogue/X_test.csv.zip')\n",
    "\n",
    "def clean_labels(text):\n",
    "    text = re.sub('-', '', text)\n",
    "    return text \n",
    "\n",
    "train.genres = train.genres.map(lambda x: clean_labels(x))\n",
    "label_vectorizer = CountVectorizer()\n",
    "train_y = label_vectorizer.fit_transform(train.genres).todense()\n",
    "label_cols = label_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.replace(\"' \", \" ' \")\n",
    "    signs = set(',.:;\"?!')\n",
    "    prods = set(text) & signs\n",
    "    if not prods:\n",
    "        return text\n",
    "\n",
    "    for sign in prods:\n",
    "        text = text.replace(sign, ' {} '.format(sign) )\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_docs(df, n_gram_max=2):\n",
    "    def add_ngram(q, n_gram_max):\n",
    "            ngrams = []\n",
    "            for n in range(2, n_gram_max+1):\n",
    "                for w_index in range(len(q)-n+1):\n",
    "                    ngrams.append('--'.join(q[w_index:w_index+n]))\n",
    "            return q + ngrams\n",
    "        \n",
    "    docs = []\n",
    "    for doc in df.dialogue:\n",
    "        doc = preprocess(doc).split()\n",
    "        docs.append(' '.join(add_ngram(doc, n_gram_max)))\n",
    "    \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_count = 2\n",
    "\n",
    "docs = create_docs(train)\n",
    "docs_test = create_docs(test)\n",
    "\n",
    "tokenizer = Tokenizer(lower=False, filters='')\n",
    "tokenizer.fit_on_texts(docs)\n",
    "num_words = sum([1 for _, v in tokenizer.word_counts.items() if v >= min_count])\n",
    "\n",
    "docs = tokenizer.texts_to_sequences(docs)\n",
    "docs_test = tokenizer.texts_to_sequences(docs_test)\n",
    "\n",
    "maxlen = 256\n",
    "docs = pad_sequences(sequences=docs, maxlen=maxlen)\n",
    "docs_test = pad_sequences(sequences=docs_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = np.max(docs) + 1\n",
    "embedding_dims = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RocAucEvaluation(Callback):\n",
    "    \n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        if not ('val_auc' in self.params['metrics']):\n",
    "            self.params['metrics'].append('roc_auc_val')        \n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        logs['val_auc'] = float('-inf')\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            logs['val_auc'] = score\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))\n",
    "\n",
    "def get_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=input_dim, output_dim=embedding_dims))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])   \n",
    "    return model\n",
    "\n",
    "def val_run_model(train_X, train_y, val_X, val_y, test_X, params): \n",
    "\n",
    "    try: pp = multiprocess.current_process()._identity[0]\n",
    "    except IndexError: pp = 0\n",
    "    \n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.Session(config=config) as sess:\n",
    "            K.set_session(sess)\n",
    "            \n",
    "            model = get_model()\n",
    "            file_path, log_file_path = \\\n",
    "                'ft_auc_checkpoint_{}.hdf5'.format(str(pp)), \\\n",
    "                'ft_csv_log_{}.csv'.format(str(pp))\n",
    "                \n",
    "            rocauc = RocAucEvaluation(validation_data=(val_X, val_y), interval=1)\n",
    "            lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=3, min_lr=0.5e-6)\n",
    "            early_stopper = EarlyStopping(monitor='val_auc', mode='max', patience=6)\n",
    "            csv_logger = CSVLogger(log_file_path)\n",
    "            checkpoint = ModelCheckpoint(file_path, monitor='val_auc', verbose=1, save_best_only=True, mode='max')           \n",
    "            callbacks_list = [rocauc, lr_reducer, early_stopper, csv_logger, checkpoint]\n",
    "            \n",
    "            model.fit(train_X, train_y, batch_size=params['bs'], epochs=40, validation_data=(val_X, val_y), verbose=2, callbacks=callbacks_list)\n",
    "            model.load_weights(file_path)\n",
    "            pred_val_y = model.predict(val_X, batch_size=512)\n",
    "            pred_test_y = model.predict(test_X, batch_size=512)        \n",
    "\n",
    "            sess.close()\n",
    "            del sess, model, lr_reducer, early_stopper, checkpoint, csv_logger; gc.collect();\n",
    "    return pred_val_y, pred_test_y                  \n",
    "  \n",
    "def val_predict_fold(i, dev_index, val_index, params): \n",
    "    pred_train = np.zeros(len(docs))\n",
    "    dev_X, val_X = docs[dev_index], docs[val_index]\n",
    "    dev_y, val_y = np.squeeze(np.array(train_y[:, i][dev_index])), np.squeeze(np.array(train_y[:, i][val_index]))\n",
    "    pred_val_y, pred_test_y = val_run_model(dev_X, dev_y, val_X, val_y, docs_test, params)\n",
    "    pred_train[val_index] = np.squeeze(np.array(pred_val_y))\n",
    "    cv_scores = roc_auc_score(val_y, pred_val_y)\n",
    "    return cv_scores, pred_train, pred_test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_predict_oof(i, n, params):  \n",
    "    if __name__ == '__main__':\n",
    "        kf = StratifiedKFold(n_splits=n, shuffle=True, random_state=0)\n",
    "        with multiprocess.Pool(n) as p:\n",
    "            r = list(p.imap(lambda x: val_predict_fold(i, x[0], x[1], params), kf.split(train_y[:, i], train_y[:, i]), chunksize=1))\n",
    "    return r, np.mean([x[0] for x in r]), np.sum([x[1] for x in r], axis=0), np.sum([x[2] for x in r], axis=0) / float(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'bs': 256,\n",
    "}\n",
    "\n",
    "preds_test = np.zeros((docs_test.shape[0], len(label_cols)))\n",
    "preds_train = np.zeros((docs.shape[0], len(label_cols)))\n",
    "cv_scores = list()\n",
    "rs = []\n",
    "\n",
    "for i, j in enumerate(label_cols):\n",
    "    print('cv', j)\n",
    "    r, scores, pred_train, pred_test = val_predict_oof(i, 5, params)\n",
    "    preds_train[:, i] = pred_train\n",
    "    preds_test[:, i] = pred_test.flatten()\n",
    "    cv_scores.append(np.mean(scores))\n",
    "    print(np.mean(scores))\n",
    "    \n",
    "print(np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    train['pred_ft0_{}'.format(str(i))] = preds_train[:, i]\n",
    "    test['pred_ft0_{}'.format(str(i))] = preds_test[:, i]\n",
    "    \n",
    "train.iloc[:, -20:].to_csv('predictions/pred_train_ft0.csv', index=False)\n",
    "test.iloc[:, -20:].to_csv('predictions/pred_test_ft0.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
